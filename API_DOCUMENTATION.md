# ğŸš€ API Documentation

## Wine Recommendation API

FastAPI backend that connects the Next.js frontend with the Python recommendation engine.

## ğŸ“ Base URL

```
http://localhost:8000
```

## ğŸ”Œ Endpoints

### 1. Health Check
**GET** `/`

Returns API status and configuration.

**Response:**
```json
{
  "status": "online",
  "service": "Wine Recommendation API",
  "version": "1.0.0",
  "llm_available": true,
  "dishes_count": 100,
  "wines_count": 28
}
```

---

### 2. Detailed Health Check
**GET** `/health`

Returns detailed system health information.

**Response:**
```json
{
  "status": "healthy",
  "llm_configured": true,
  "database": {
    "pratos": 100,
    "vinhos": 28
  }
}
```

---

### 3. Wine Recommendation (Main Endpoint)
**POST** `/api/recomendacao`

Get wine recommendation with AI-generated justification.

**Request Body:**
```json
{
  "mensagem": "Sushi"
}
```

**Response:**
```json
{
  "prato": "Sushi",
  "vinho": {
    "nome": "Pinot Grigio",
    "tipo": "branco seco",
    "similaridade": 98.14,
    "score_features": 95.35,
    "score_regras": 100.0
  },
  "justificativa": "O Pinot Grigio harmoniza perfeitamente com Sushi devido Ã  sua acidez refrescante...",
  "mensagem": "ğŸ· **Pinot Grigio** (branco seco)\n\nğŸ“Š **Compatibilidade:** 98.1%\n\nâœ¨ **Justificativa:**\n..."
}
```

**Error Response (Dish Not Found):**
```json
{
  "prato": "",
  "vinho": {
    "nome": "",
    "tipo": "",
    "similaridade": 0,
    "score_features": 0,
    "score_regras": 0
  },
  "justificativa": "",
  "mensagem": "Desculpe, nÃ£o encontrei informaÃ§Ãµes sobre \"Pizza\". Tente mencionar um prato especÃ­fico..."
}
```

---

### 4. List Dishes
**GET** `/api/pratos`

Returns list of available dishes.

**Response:**
```json
{
  "total": 100,
  "pratos": [
    "Sushi",
    "SalmÃ£o grelhado",
    "Picanha na brasa",
    ...
  ],
  "message": "Total de 100 pratos disponÃ­veis"
}
```

---

### 5. List Wines
**GET** `/api/vinhos`

Returns list of all wines.

**Response:**
```json
{
  "total": 28,
  "vinhos": [
    {
      "vinho": "Cabernet Sauvignon",
      "tipo_vinho": "tinto seco"
    },
    {
      "vinho": "Pinot Grigio",
      "tipo_vinho": "branco seco"
    },
    ...
  ]
}
```

---

## ğŸ”§ How It Works

### Request Flow

```
Frontend (Next.js)
      â†“
POST /api/recomendacao
      â†“
FastAPI Backend (api.py)
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Parse dish name      â”‚
â”‚ 2. Search in CSV        â”‚
â”‚ 3. Find matches         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
Python Backend (backend/)
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ sistema_recomendacao_   â”‚
â”‚ vinho.py                â”‚
â”‚ - KNN similarity        â”‚
â”‚ - Rule-based scoring    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ llm.py                  â”‚
â”‚ - Generate              â”‚
â”‚   justification         â”‚
â”‚ - Perplexity API        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
Response to Frontend
```

### Scoring Algorithm

```python
Final Score = (40% Ã— Feature Similarity) + (60% Ã— Rule-Based Score)
```

- **Feature Similarity**: Cosine similarity between dish and wine characteristics
- **Rule-Based Score**: Expert wine-pairing knowledge

---

## ğŸš€ Running the API

### Option 1: Direct Python
```bash
python api.py
```

### Option 2: Uvicorn
```bash
uvicorn api:app --reload --host 0.0.0.0 --port 8000
```

### Option 3: With uv
```bash
uv run python api.py
```

---

## ğŸ”‘ Environment Variables

Required in `.env`:
```env
PERPLEXITY_API_KEY=your_perplexity_api_key_here
```

Optional:
```env
DEBUG=False
LOG_LEVEL=INFO
```

---

## ğŸ§ª Testing the API

### Using cURL

**Health Check:**
```bash
curl http://localhost:8000/health
```

**Get Recommendation:**
```bash
curl -X POST http://localhost:8000/api/recomendacao \
  -H "Content-Type: application/json" \
  -d '{"mensagem": "Sushi"}'
```

**List Dishes:**
```bash
curl http://localhost:8000/api/pratos
```

### Using Python

```python
import requests

# Get recommendation
response = requests.post(
    "http://localhost:8000/api/recomendacao",
    json={"mensagem": "SalmÃ£o grelhado"}
)
print(response.json())
```

### Using JavaScript/TypeScript (Frontend)

```typescript
const response = await fetch('http://localhost:8000/api/recomendacao', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({ mensagem: 'Sushi' }),
});

const data = await response.json();
console.log(data);
```

---

## ğŸ“Š Response Schema

### RecomendacaoResponse

```typescript
interface RecomendacaoResponse {
  prato: string;              // Dish name
  vinho: VinhoResponse;       // Wine details
  justificativa: string;      // AI-generated justification
  mensagem: string;           // Formatted message for display
}

interface VinhoResponse {
  nome: string;               // Wine name
  tipo: string;               // Wine type (e.g., "branco seco")
  similaridade: number;       // Overall similarity (0-100)
  score_features: number;     // Feature-based score (0-100)
  score_regras: number;       // Rule-based score (0-100)
}
```

---

## âš¡ Performance

- **Average Response Time**: 2-5 seconds (with LLM)
- **Without LLM**: < 100ms
- **Concurrent Requests**: Supports multiple simultaneous requests
- **Caching**: DataFrames cached in memory for fast access

---

## ğŸ›¡ï¸ CORS Configuration

The API allows requests from:
- `http://localhost:3000`
- `http://127.0.0.1:3000`

To add more origins, edit `api.py`:

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://your-domain.com"],
    ...
)
```

---

## ğŸ› Error Handling

### 400 Bad Request
```json
{
  "detail": "Mensagem Ã© obrigatÃ³ria"
}
```

### 500 Internal Server Error
```json
{
  "detail": "Erro ao processar recomendaÃ§Ã£o: <error message>"
}
```

---

## ğŸ“ Logs

The API logs to console:
```
ğŸ· Wine Recommendation API
================================================================================
ğŸ“Š Loaded 100 dishes and 28 wines
ğŸ¤– LLM Status: âœ… Available
================================================================================
INFO:     Started server process [12345]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000
```

---

## ğŸ”— Integration with Frontend

The Next.js frontend automatically connects to the API:

1. **Frontend runs on**: `http://localhost:3000`
2. **API runs on**: `http://localhost:8000`
3. **Frontend calls**: `http://localhost:8000/api/recomendacao`

Make sure both servers are running:
```bash
# Terminal 1: Backend API
python api.py

# Terminal 2: Frontend
npm run dev
```

---

## ğŸ“š Dependencies

- **FastAPI**: Modern web framework
- **Uvicorn**: ASGI server
- **Pydantic**: Data validation
- **pandas**: Data manipulation
- **scikit-learn**: ML algorithms
- **dspy-ai**: LLM framework

Install with:
```bash
uv sync
# or
pip install fastapi uvicorn[standard] pandas scikit-learn dspy-ai python-dotenv
```

---

## ğŸ¯ Future Improvements

- [ ] Add authentication (API keys)
- [ ] Implement rate limiting
- [ ] Add request caching (Redis)
- [ ] Support for batch requests
- [ ] WebSocket support for streaming responses
- [ ] Add Swagger/OpenAPI documentation UI
- [ ] Add metrics and monitoring
- [ ] Docker containerization

---

**Made with ğŸ· and ğŸ**
